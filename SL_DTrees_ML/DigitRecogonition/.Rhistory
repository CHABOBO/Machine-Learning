##V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 +
##  V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 +
##  V32 + V33 + V34 + V35 + V36 + V37 + V38 + V39 + V40 + V41 +
##  V42 + V43 + V44 + V45 + V46 + V47 + V48 + V49 + V50 + V51 +
##  V52 + V53 + V54 + V55 + V56 + V57 + V58 + V59 + V60 + V61 +
##  V62 + V63 + V64###########################################
xnam <- paste(colnames(train_data[2:65]), sep="", collapse = "+")
fmla <- as.formula(paste(paste(colnames(train_data[1]), " ~ ", sep = ""), xnam))
##Splitting the data training data into sets,one containg 80% rowns and another contianing 20%#####
train <- sample(1:nrow(train_data), 0.8*nrow(train_data))
input_trainData <- train_data[train, ]
input_testData <- train_data[-train, ]
## Decision Tree creation using rpart package without pruning by using information gain.##############################
rpartMod <- rpart(fmla, data = input_trainData, method = "class",parms = list(split = 'information'), minsplit = 2, minbucket = 1)
plot(rpartMod)
text(rpartMod, pretty = 400)
##Tree has been pruned by using the prune method in rpart package##########################################
treePruned_inputtrain <- prune(rpartMod,cp = rpartMod$cptable[which.min(rpartMod$cptable[,"xerror"]), "CP"])
plot(treePruned_inputtrain)
text(treePruned_inputtrain, pretty = 400)
#####################################################################################################
######Running the decision tree obtianed on the cross-validation data(remaining 20% of the data)#####
######Output is stored on the file  file = "input_test_result.csv"###################################
out <- predict(treePruned_inputtrain, newdata = input_testData, type="class") #Returns the prediction
input_testData$Output <- out
input_testData <- input_testData[, c(66, 1:65)]
write.csv(input_testData, file = "input_test_result.csv")
##############Now Testing the Data on the actual test data###########################################
################Output is stored on the file test_result.csv#########################################
test_data <- read.csv("optdigits_test.csv", header = FALSE)
test_data <- test_data[, c(65, 1:64)]
outtest <- predict(treePruned_inputtrain, newdata = test_data, type="class") #Returns the predicted class
test_data$Output <- outtest
test_data <- test_data[, c(66, 1:65)]
write.csv(test_data, file = "final_test_result.csv")
pred_test.response <- colnames(outtest)[max.col(outtest, ties.method = c("random"))]
###################################End of the Program#########################################################################
summary(train_data)
sd(train_data)
sd(train_data$V65)
apply(train_data,2,sd)
sd(train_data$V1)
appply(train_data,2,mean)
apply(train_data,2,mean)
apply(train_data,2,sd)
apply(train_data,2,mean)
apply(train_data,3,mean)
a <- table(train_data)
count(train_data$V65)
count(train_data$V65)
install.packages('plyr')
install.packages("plyr")
count(train_data$V65)
library(plyr)
library("plyr")
library(plyr)
install.packages("plyr")
install.packages("plyr")
library(plyr)
count(train_data$V65)
count(test_data$V65)
rsq.rpart(rpartMod)
library(rpart)
rsq.rpart(rpartMod)
##############################
####Author:Febin zachariah####
####Date:01/29/2017###########
#######################################
## packages needed for implementation:#
## install.packages("rpart")###########
## install.packages("rattle")##########
## install.packages("rpart.plot")######
## install.packages("RColorBrewer")####
#######################################
######Importing the libraries used#####
library("rpart")
library("rattle")
library("rpart.plot")
library("RColorBrewer")
library("party")
library("partykit")
library("caret")
#######Loading the required File for creating the decision tree#########
train_data <- read.csv("optdigits_raining.csv", header = FALSE)
train_data <- train_data[, c(65, 1:64)]
###Creating the formula for showing the dependeny of input attributes to output attribute#####
##############################################################################################
#V65 ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 +
##V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 +
##  V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 +
##  V32 + V33 + V34 + V35 + V36 + V37 + V38 + V39 + V40 + V41 +
##  V42 + V43 + V44 + V45 + V46 + V47 + V48 + V49 + V50 + V51 +
##  V52 + V53 + V54 + V55 + V56 + V57 + V58 + V59 + V60 + V61 +
##  V62 + V63 + V64###########################################
xnam <- paste(colnames(train_data[2:65]), sep="", collapse = "+")
fmla <- as.formula(paste(paste(colnames(train_data[1]), " ~ ", sep = ""), xnam))
##Splitting the data training data into sets,one containg 80% rowns and another contianing 20%#####
train <- sample(1:nrow(train_data), 0.5*nrow(train_data))
input_trainData <- train_data[train, ]
input_testData <- train_data[-train, ]
## Decision Tree creation using rpart package without pruning by using information gain.##############################
rpartMod <- rpart(fmla, data = input_trainData, method = "class",parms = list(split = 'information'), minsplit = 2, minbucket = 1)
plot(rpartMod)
text(rpartMod, pretty = 400)
##Tree has been pruned by using the prune method in rpart package##########################################
treePruned_inputtrain <- prune(rpartMod,cp = rpartMod$cptable[which.min(rpartMod$cptable[,"xerror"]), "CP"])
plot(treePruned_inputtrain)
text(treePruned_inputtrain, pretty = 400)
#####################################################################################################
######Running the decision tree obtianed on the cross-validation data(remaining 20% of the data)#####
######Output is stored on the file  file = "input_test_result.csv"###################################
out <- predict(treePruned_inputtrain, newdata = input_testData, type="class") #Returns the prediction
input_testData$Output <- out
input_testData <- input_testData[, c(66, 1:65)]
write.csv(input_testData, file = "input_test_result.csv")
##############Now Testing the Data on the actual test data###########################################
################Output is stored on the file test_result.csv#########################################
test_data <- read.csv("optdigits_test.csv", header = FALSE)
test_data <- test_data[, c(65, 1:64)]
outtest <- predict(treePruned_inputtrain, newdata = test_data, type="class") #Returns the predicted class
test_data$Output <- outtest
test_data <- test_data[, c(66, 1:65)]
write.csv(test_data, file = "final_test_result.csv")
pred_test.response <- colnames(outtest)[max.col(outtest, ties.method = c("random"))]
rsq.rpart(rpartMod)
###################################End of the Program#########################################################################
source('C:/Users/febin/Desktop/SL_DTrees_ML/AmazonReview/review_sentiment.R')
library("rpart")
library("plyr")
library("stringr")
##############################
####Author:Febin Zachariah####
####Date:02/03/2017###########
#######################################
## packages needed for implementation:#
## install.packages("rpart")###########
## install.packages("plyr")##########
## install.packages("stringr")######
#######################################
######Importing the libraries used#####
library("rpart")
library("plyr")
library("stringr")
##Following function is used to find the sentiment score of each review beased on the number of postivie and negative words######
##Postive Words are stored in the file named:positive-words.txt##################################################################
##Negative Words are stored in the file named:negative-words.txt#################################################################
score.sentiment <- function(sentences, pos.words, neg.words, .progress = 'none')
{
require(plyr)
require(stringr)
scores <- laply(sentences, function(sentence, pos.words, neg.words){
sentence <- gsub('[[:punct:]]', "", sentence)
sentence <- gsub('[[:cntrl:]]', "", sentence)
sentence <- gsub('\\d+', "", sentence)
sentence <- tolower(sentence)
word.list <- str_split(sentence, '\\s+')
words <- unlist(word.list)
pos.matches <- match(words, pos.words)
neg.matches <- match(words, neg.words)
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
score <- sum(pos.matches) - sum(neg.matches)
return(c(score, sum(pos.matches), sum(neg.matches)))
}, pos.words, neg.words, .progress = .progress)
scores.df <- data.frame(score = scores[,1], text = sentences, pos_count = scores[,2], neg_count = scores[,3])
return(scores.df)
}
##Reading the positive and negative words from textfiles#######################
positive_words <- scan('positive-words.txt', what='character', comment.char=';')
negative_words <- scan('negative-words.txt', what='character', comment.char=';')
##Reading the training Data#########################################################
amazon_train_dataset <- read.csv('amazon_baby_train.csv', header = TRUE)
amazon_train_dataset$review <- as.factor(amazon_train_dataset$review)
##finding the sentiment score ,number of positive words,number of negative words by calling the score.sentiment function#####
scores <- score.sentiment(amazon_train_dataset$review, positive_words, negative_words, .progress='text')
amazon_train_dataset$score <- scores[,1]
amazon_train_dataset$pos_count <- scores[,3]
amazon_train_dataset$neg_count <- scores[,4]
##Splitting the data to get cross validation dataset#####################################################################
train <- sample(1:nrow(amazon_train_dataset), 0.8*nrow(amazon_train_dataset))
input_trainData <- amazon_train_dataset[train, ]
input_testData <- amazon_train_dataset[-train, ]
####creating the decision tree by using rpart library
rpartMod <- rpart(rating ~ score + pos_count + neg_count, data = input_trainData, method = "class",parms = list(split = 'information'), minsplit = 2, minbucket = 1)
treePruned_inputtrain <- prune(rpartMod,cp = rpartMod$cptable[which.min(rpartMod$cptable[,"xerror"]), "CP"])
##Running the decision tree on the cross-validation data
out <- predict(treePruned_inputtrain, newdata = input_testData, type="class")
input_testData$Output <- out
write.csv(input_testData, file = "amazon_traintestdata_result.csv")
##Running the decision tree learning on the actual dataset
amazon_test_data <- read.csv("amazon_baby_test.csv", header = TRUE)
amazon_test_data$review <- as.factor(amazon_test_data$review)
test_scores <- score.sentiment(amazon_test_data$review, positive_words, negative_words, .progress='text')
amazon_test_data$score <- test_scores[,1]
amazon_test_data$pos_count <- test_scores[,3]
amazon_test_data$neg_count <- test_scores[,4]
outtest <- predict(treePruned_inputtrain, newdata = amazon_test_data, type="class")
amazon_test_data$Output <- outtest
write.csv(amazon_test_data, file = "amazon_testdata_result.csv")
library("rpart")
library("plyr")
library("stringr")
score.sentiment <- function(sentences, pos.words, neg.words, .progress = 'none')
{
require(plyr)
require(stringr)
scores <- laply(sentences, function(sentence, pos.words, neg.words){
sentence <- gsub('[[:punct:]]', "", sentence)
sentence <- gsub('[[:cntrl:]]', "", sentence)
sentence <- gsub('\\d+', "", sentence)
sentence <- tolower(sentence)
word.list <- str_split(sentence, '\\s+')
words <- unlist(word.list)
pos.matches <- match(words, pos.words)
neg.matches <- match(words, neg.words)
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
score <- sum(pos.matches) - sum(neg.matches)
return(c(score, sum(pos.matches), sum(neg.matches)))
}, pos.words, neg.words, .progress = .progress)
scores.df <- data.frame(score = scores[,1], text = sentences, pos_count = scores[,2], neg_count = scores[,3])
return(scores.df)
}
##Reading the positive and negative words from textfiles#######################
positive_words <- scan('positive-words.txt', what='character', comment.char=';')
negative_words <- scan('negative-words.txt', what='character', comment.char=';')
getwd()
setwd(C:/Users/febin/Desktop/SL_DTrees_ML/AmazonReview)
setwd("C:/Users/febin/Desktop/SL_DTrees_ML/AmazonReview")
##Reading the positive and negative words from textfiles#######################
positive_words <- scan('positive-words.txt', what='character', comment.char=';')
negative_words <- scan('negative-words.txt', what='character', comment.char=';')
#Reading the training Data#########################################################
amazon_train_dataset <- read.csv('amazon_baby_train.csv', header = TRUE)
amazon_train_dataset$review <- as.factor(amazon_train_dataset$review)
##finding the sentiment score ,number of positive words,number of negative words by calling the score.sentiment function#####
scores <- score.sentiment(amazon_train_dataset$review, positive_words, negative_words, .progress='text')
amazon_train_dataset$score <- scores[,1]
amazon_train_dataset$pos_count <- scores[,3]
amazon_train_dataset$neg_count <- scores[,4]
train <- sample(1:nrow(amazon_train_dataset), 0.8*nrow(amazon_train_dataset))
input_trainData <- amazon_train_dataset[train, ]
input_testData <- amazon_train_dataset[-train, ]
rpartMod <- rpart(rating ~ score + pos_count + neg_count, data = input_trainData, method = "class",parms = list(split = 'information'), minsplit = 2, minbucket = 1)
treePruned_inputtrain <- prune(rpartMod,cp = rpartMod$cptable[which.min(rpartMod$cptable[,"xerror"]), "CP"])
out <- predict(treePruned_inputtrain, newdata = input_testData, type="class")
input_testData$Output <- out
write.csv(input_testData, file = "amazon_traintestdata_result.csv")
amazon_test_data <- read.csv("amazon_baby_test.csv", header = TRUE)
amazon_test_data$review <- as.factor(amazon_test_data$review)
test_scores <- score.sentiment(amazon_test_data$review, positive_words, negative_words, .progress='text')
amazon_test_data$score <- test_scores[,1]
amazon_test_data$pos_count <- test_scores[,3]
amazon_test_data$neg_count <- test_scores[,4]
outtest <- predict(treePruned_inputtrain, newdata = amazon_test_data, type="class")
amazon_test_data$Output <- outtest
write.csv(amazon_test_data, file = "amazon_testdata_result.csv")
summary(amazon_train_dataset)
sd(amazon_train_dataset$rating)
summary(amazon_test_data)
sd(amazon_test_data$rating)
count(amazon_test_data)
count(amazon_train_dataset$rating)
count(amazon_test_data$rating)
##############################
####Author:Febin zachariah####
####Date:01/29/2017###########
#######################################
## packages needed for implementation:#
## install.packages("rpart")###########
## install.packages("rattle")##########
## install.packages("rpart.plot")######
## install.packages("RColorBrewer")####
#######################################
######Importing the libraries used#####
library("rpart")
library("rattle")
library("rpart.plot")
library("RColorBrewer")
library("party")
library("partykit")
library("caret")
#######Loading the required File for creating the decision tree#########
train_data <- read.csv("optdigits_raining.csv", header = FALSE)
train_data <- train_data[, c(65, 1:64)]
###Creating the formula for showing the dependeny of input attributes to output attribute#####
##############################################################################################
#V65 ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 +
##V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 +
##  V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 +
##  V32 + V33 + V34 + V35 + V36 + V37 + V38 + V39 + V40 + V41 +
##  V42 + V43 + V44 + V45 + V46 + V47 + V48 + V49 + V50 + V51 +
##  V52 + V53 + V54 + V55 + V56 + V57 + V58 + V59 + V60 + V61 +
##  V62 + V63 + V64###########################################
xnam <- paste(colnames(train_data[2:65]), sep="", collapse = "+")
fmla <- as.formula(paste(paste(colnames(train_data[1]), " ~ ", sep = ""), xnam))
##Splitting the data training data into sets,one containg 80% rowns and another contianing 20%#####
train <- sample(1:nrow(train_data), 0.5*nrow(train_data))
input_trainData <- train_data[train, ]
input_testData <- train_data[-train, ]
## Decision Tree creation using rpart package without pruning by using information gain.##############################
rpartMod <- rpart(fmla, data = input_trainData, method = "class",parms = list(split = 'information'), minsplit = 2, minbucket = 1)
plot(rpartMod)
text(rpartMod, pretty = 400)
##Tree has been pruned by using the prune method in rpart package##########################################
treePruned_inputtrain <- prune(rpartMod,cp = -1)
plot(treePruned_inputtrain)
text(treePruned_inputtrain, pretty = 400)
#####################################################################################################
######Running the decision tree obtianed on the cross-validation data(remaining 20% of the data)#####
######Output is stored on the file  file = "input_test_result.csv"###################################
out <- predict(treePruned_inputtrain, newdata = input_testData, type="class") #Returns the prediction
input_testData$Output <- out
input_testData <- input_testData[, c(66, 1:65)]
write.csv(input_testData, file = "input_test_result.csv")
##############Now Testing the Data on the actual test data###########################################
################Output is stored on the file test_result.csv#########################################
test_data <- read.csv("optdigits_test.csv", header = FALSE)
test_data <- test_data[, c(65, 1:64)]
outtest <- predict(treePruned_inputtrain, newdata = test_data, type="class") #Returns the predicted class
test_data$Output <- outtest
test_data <- test_data[, c(66, 1:65)]
write.csv(test_data, file = "final_test_result.csv")
pred_test.response <- colnames(outtest)[max.col(outtest, ties.method = c("random"))]
###################################End of the Program#########################################################################
rpartMod <- rpart(fmla, data = input_trainData, method = "class",parms = list(split = 'information'), minsplit = 2, minbucket = 1)
##############################
####Author:Febin zachariah####
####Date:01/29/2017###########
#######################################
## packages needed for implementation:#
## install.packages("rpart")###########
## install.packages("rattle")##########
## install.packages("rpart.plot")######
## install.packages("RColorBrewer")####
#######################################
######Importing the libraries used#####
library("rpart")
library("rattle")
library("rpart.plot")
library("RColorBrewer")
library("party")
library("partykit")
library("caret")
#######Loading the required File for creating the decision tree#########
train_data <- read.csv("optdigits_raining.csv", header = FALSE)
train_data <- train_data[, c(65, 1:64)]
###Creating the formula for showing the dependeny of input attributes to output attribute#####
##############################################################################################
#V65 ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 +
##V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 +
##  V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 +
##  V32 + V33 + V34 + V35 + V36 + V37 + V38 + V39 + V40 + V41 +
##  V42 + V43 + V44 + V45 + V46 + V47 + V48 + V49 + V50 + V51 +
##  V52 + V53 + V54 + V55 + V56 + V57 + V58 + V59 + V60 + V61 +
##  V62 + V63 + V64###########################################
xnam <- paste(colnames(train_data[2:65]), sep="", collapse = "+")
fmla <- as.formula(paste(paste(colnames(train_data[1]), " ~ ", sep = ""), xnam))
##Splitting the data training data into sets,one containg 80% rowns and another contianing 20%#####
train <- sample(1:nrow(train_data), 0.5*nrow(train_data))
input_trainData <- train_data[train, ]
input_testData <- train_data[-train, ]
## Decision Tree creation using rpart package without pruning by using information gain.##############################
rpartMod <- rpart(fmla, data = input_trainData, method = "class",parms = list(split = 'information'), minsplit = 2, minbucket = 1)
plot(rpartMod)
text(rpartMod, pretty = 400)
##Tree has been pruned by using the prune method in rpart package##########################################
treePruned_inputtrain <- prune(rpartMod,cp = -2)
plot(treePruned_inputtrain)
text(treePruned_inputtrain, pretty = 400)
#####################################################################################################
######Running the decision tree obtianed on the cross-validation data(remaining 20% of the data)#####
######Output is stored on the file  file = "input_test_result.csv"###################################
out <- predict(treePruned_inputtrain, newdata = input_testData, type="class") #Returns the prediction
input_testData$Output <- out
input_testData <- input_testData[, c(66, 1:65)]
write.csv(input_testData, file = "input_test_result.csv")
##############Now Testing the Data on the actual test data###########################################
################Output is stored on the file test_result.csv#########################################
test_data <- read.csv("optdigits_test.csv", header = FALSE)
test_data <- test_data[, c(65, 1:64)]
outtest <- predict(treePruned_inputtrain, newdata = test_data, type="class") #Returns the predicted class
test_data$Output <- outtest
test_data <- test_data[, c(66, 1:65)]
write.csv(test_data, file = "final_test_result.csv")
pred_test.response <- colnames(outtest)[max.col(outtest, ties.method = c("random"))]
###################################End of the Program#########################################################################
##############################
####Author:Febin zachariah####
####Date:01/29/2017###########
#######################################
## packages needed for implementation:#
## install.packages("rpart")###########
## install.packages("rattle")##########
## install.packages("rpart.plot")######
## install.packages("RColorBrewer")####
#######################################
######Importing the libraries used#####
library("rpart")
library("rattle")
library("rpart.plot")
library("RColorBrewer")
library("party")
library("partykit")
library("caret")
#######Loading the required File for creating the decision tree#########
train_data <- read.csv("optdigits_raining.csv", header = FALSE)
train_data <- train_data[, c(65, 1:64)]
###Creating the formula for showing the dependeny of input attributes to output attribute#####
##############################################################################################
#V65 ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 +
##V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 +
##  V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 +
##  V32 + V33 + V34 + V35 + V36 + V37 + V38 + V39 + V40 + V41 +
##  V42 + V43 + V44 + V45 + V46 + V47 + V48 + V49 + V50 + V51 +
##  V52 + V53 + V54 + V55 + V56 + V57 + V58 + V59 + V60 + V61 +
##  V62 + V63 + V64###########################################
xnam <- paste(colnames(train_data[2:65]), sep="", collapse = "+")
fmla <- as.formula(paste(paste(colnames(train_data[1]), " ~ ", sep = ""), xnam))
##Splitting the data training data into sets,one containg 80% rowns and another contianing 20%#####
train <- sample(1:nrow(train_data), 0.5*nrow(train_data))
input_trainData <- train_data[train, ]
input_testData <- train_data[-train, ]
## Decision Tree creation using rpart package without pruning by using information gain.##############################
rpartMod <- rpart(fmla, data = input_trainData, method = "class",parms = list(split = 'information'), minsplit = 2, minbucket = 1)
plot(rpartMod)
text(rpartMod, pretty = 400)
##Tree has been pruned by using the prune method in rpart package##########################################
treePruned_inputtrain <- prune(rpartMod,cp = rpartMod$cptable[which.min(rpartMod$cptable[,"xerror"]), "CP"])
plot(treePruned_inputtrain)
text(treePruned_inputtrain, pretty = 400)
#####################################################################################################
######Running the decision tree obtianed on the cross-validation data(remaining 20% of the data)#####
######Output is stored on the file  file = "input_test_result.csv"###################################
out <- predict(treePruned_inputtrain, newdata = input_testData, type="class") #Returns the prediction
input_testData$Output <- out
input_testData <- input_testData[, c(66, 1:65)]
write.csv(input_testData, file = "input_test_result.csv")
##############Now Testing the Data on the actual test data###########################################
################Output is stored on the file test_result.csv#########################################
test_data <- read.csv("optdigits_test.csv", header = FALSE)
test_data <- test_data[, c(65, 1:64)]
outtest <- predict(treePruned_inputtrain, newdata = test_data, type="class") #Returns the predicted class
test_data$Output <- outtest
test_data <- test_data[, c(66, 1:65)]
write.csv(test_data, file = "final_test_result.csv")
pred_test.response <- colnames(outtest)[max.col(outtest, ties.method = c("random"))]
###################################End of the Program#########################################################################
getwd()
setwd("C:/Users/febin/Desktop/SL_DTrees_ML/DigitalRecogonition")
setwd("C:/Users/febin/Desktop/SL_DTrees_ML/DigitRecogonition")
##############################
####Author:Febin zachariah####
####Date:01/29/2017###########
#######################################
## packages needed for implementation:#
## install.packages("rpart")###########
## install.packages("rattle")##########
## install.packages("rpart.plot")######
## install.packages("RColorBrewer")####
#######################################
######Importing the libraries used#####
library("rpart")
library("rattle")
library("rpart.plot")
library("RColorBrewer")
library("party")
library("partykit")
library("caret")
#######Loading the required File for creating the decision tree#########
train_data <- read.csv("optdigits_raining.csv", header = FALSE)
train_data <- train_data[, c(65, 1:64)]
###Creating the formula for showing the dependeny of input attributes to output attribute#####
##############################################################################################
#V65 ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 +
##V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 +
##  V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 +
##  V32 + V33 + V34 + V35 + V36 + V37 + V38 + V39 + V40 + V41 +
##  V42 + V43 + V44 + V45 + V46 + V47 + V48 + V49 + V50 + V51 +
##  V52 + V53 + V54 + V55 + V56 + V57 + V58 + V59 + V60 + V61 +
##  V62 + V63 + V64###########################################
xnam <- paste(colnames(train_data[2:65]), sep="", collapse = "+")
fmla <- as.formula(paste(paste(colnames(train_data[1]), " ~ ", sep = ""), xnam))
##Splitting the data training data into sets,one containg 80% rowns and another contianing 20%#####
train <- sample(1:nrow(train_data), 0.5*nrow(train_data))
input_trainData <- train_data[train, ]
input_testData <- train_data[-train, ]
## Decision Tree creation using rpart package without pruning by using information gain.##############################
rpartMod <- rpart(fmla, data = input_trainData, method = "class",parms = list(split = 'information'), minsplit = 2, minbucket = 1)
plot(rpartMod)
text(rpartMod, pretty = 400)
##Tree has been pruned by using the prune method in rpart package##########################################
treePruned_inputtrain <- prune(rpartMod,cp = -1)
plot(treePruned_inputtrain)
text(treePruned_inputtrain, pretty = 400)
#####################################################################################################
######Running the decision tree obtianed on the cross-validation data(remaining 20% of the data)#####
######Output is stored on the file  file = "input_test_result.csv"###################################
out <- predict(treePruned_inputtrain, newdata = input_testData, type="class") #Returns the prediction
input_testData$Output <- out
input_testData <- input_testData[, c(66, 1:65)]
write.csv(input_testData, file = "input_test_result.csv")
##############Now Testing the Data on the actual test data###########################################
################Output is stored on the file test_result.csv#########################################
test_data <- read.csv("optdigits_test.csv", header = FALSE)
test_data <- test_data[, c(65, 1:64)]
outtest <- predict(treePruned_inputtrain, newdata = test_data, type="class") #Returns the predicted class
test_data$Output <- outtest
test_data <- test_data[, c(66, 1:65)]
write.csv(test_data, file = "final_test_result.csv")
pred_test.response <- colnames(outtest)[max.col(outtest, ties.method = c("random"))]
###################################End of the Program#########################################################################
